{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ActorCriticModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import operator\n",
    "from TorinoEnvironment import TorinoEnvironment\n",
    "import import_ipynb\n",
    "import ActorCriticModel\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find correct hyperparameters\n",
    "hidden_size = 32 \n",
    "learning_rate = 1e-3 \n",
    "max_episodes = 200\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, learning_rate, state_size, action_size, hidden_size, max_episodes, gamma):\n",
    "    actor = ActorCriticModel.Actor(state_size, action_size, hidden_size).double()\n",
    "    critic = ActorCriticModel.Critic(state_size, action_size, hidden_size).double()    \n",
    "    \n",
    "    params = list(actor.parameters()) + list(critic.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "    \n",
    "    rewards_episodes_avg = []\n",
    "    rewards_episodes_min = []\n",
    "    rewards_episodes_max = []\n",
    "    \n",
    "    losses_actor = []\n",
    "    losses_critic = []\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        print(\"Episode #\", episode)\n",
    "        env.resetState()\n",
    "        state = np.array([env.minimumNrOfCPUs])\n",
    "\n",
    "        rewards = []\n",
    "        values = []\n",
    "        action_probs = []\n",
    "        rewards_min = 0.0\n",
    "        rewards_max = 0.0\n",
    "\n",
    "        while not env.stop:\n",
    "            state = torch.from_numpy(state).double()\n",
    "            action_dist = actor.forward(state)\n",
    "    \n",
    "            value = critic.forward(state)\n",
    "            values.append(value)\n",
    "            \n",
    "            action = np.random.choice(action_size, p=np.squeeze(action_dist.detach().numpy()))\n",
    "            action_probs.append(action_dist[action])\n",
    "            state = np.array([env.evolveState(nrOfCPUs=action)])\n",
    "            reward = env.getReward()\n",
    "            rewards.append(reward)\n",
    "            if reward < rewards_min:\n",
    "                rewards_min = reward\n",
    "            elif reward > rewards_max:\n",
    "                rewards_max = reward\n",
    "\n",
    "        print(\"min reward episode: \", rewards_min)\n",
    "        print(\"max reward episode: \", rewards_max)\n",
    "        discounted_sum = 0.0\n",
    "        returns = []\n",
    "\n",
    "        rewards_episodes_avg.append(np.sum(rewards)/env.duration) \n",
    "        rewards_episodes_min.append(np.amin(rewards))\n",
    "        rewards_episodes_max.append(np.amax(rewards))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_actor, loss_critic = ActorCriticModel.calc_loss(torch.stack(action_probs), \n",
    "                                          torch.cat(values), \n",
    "                                          torch.tensor(rewards).double(),\n",
    "                                          gamma)\n",
    "\n",
    "        losses_actor.append(loss_actor.item())\n",
    "        losses_critic.append(loss_critic.item())\n",
    "        \n",
    "        loss = loss_actor + loss_critic\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"total loss \",loss, \"actor loss \", loss_actor, \"critic loss \", loss_critic)\n",
    "        losses_actor.append(loss_actor.item())\n",
    "        losses_critic.append(loss_critic.item())\n",
    "    \n",
    "    #Uncomment this if you want to save your progress!\n",
    "    #torch.save(actor.state_dict(), 'actor_state_dict')\n",
    "    #torch.save(critic.state_dict(), 'critic_state_dict')\n",
    "    \n",
    "    plt.plot(losses_actor, label=\"loss actor\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_actor_torino\")\n",
    "    plt.show()\n",
    "    plt.plot(losses_critic, label=\"loss critic\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_critic_torino\")\n",
    "    plt.show()\n",
    "    return rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max):    \n",
    "    #smoothed_rewards = pd.Series.rolling(pd.Series(rewards_episodes), 10).mean() # hier zou alle rewards weer meegenomen moeten worden\n",
    "    #smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "    \n",
    "    plt.plot(rewards_episodes_avg, label=\"Average reward per episode\")\n",
    "    plt.plot(rewards_episodes_min, label=\"Minimum reward per episode\")\n",
    "    plt.plot(rewards_episodes_max, label=\"Maximum reward per episode\")\n",
    "    #plt.plot(smoothed_rewards)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55659\n",
      "13915\n"
     ]
    }
   ],
   "source": [
    "train_env, test_env = TorinoEnvironment(), TorinoEnvironment()\n",
    "train_env.head(.8)\n",
    "test_env.tail(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode # 0\n",
      "min reward episode:  -0.9094967544864451\n",
      "max reward episode:  0.4236930567491761\n",
      "total loss  tensor(4278795.9478, dtype=torch.float64, grad_fn=<AddBackward0>) actor loss  tensor(4278231.3055, dtype=torch.float64, grad_fn=<NegBackward>) critic loss  tensor(564.6423, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "Episode # 1\n",
      "min reward episode:  -1.2395311187476135\n",
      "max reward episode:  0.4319746173196366\n",
      "total loss  tensor(4345359.3126, dtype=torch.float64, grad_fn=<AddBackward0>) actor loss  tensor(4344807.7800, dtype=torch.float64, grad_fn=<NegBackward>) critic loss  tensor(551.5326, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "Episode # 2\n",
      "min reward episode:  -1.07818098510882\n",
      "max reward episode:  0.4143442450116413\n",
      "total loss  tensor(4353763.9552, dtype=torch.float64, grad_fn=<AddBackward0>) actor loss  tensor(4353225.0665, dtype=torch.float64, grad_fn=<NegBackward>) critic loss  tensor(538.8887, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "Episode # 3\n",
      "min reward episode:  -0.9094967544864451\n",
      "max reward episode:  0.4143483490404265\n",
      "total loss  tensor(4290407.4741, dtype=torch.float64, grad_fn=<AddBackward0>) actor loss  tensor(4289885.3631, dtype=torch.float64, grad_fn=<NegBackward>) critic loss  tensor(522.1110, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "Episode # 4\n",
      "min reward episode:  -1.4412187857961052\n",
      "max reward episode:  0.4192132485895639\n",
      "total loss  tensor(4240250.8030, dtype=torch.float64, grad_fn=<AddBackward0>) actor loss  tensor(4239742.3726, dtype=torch.float64, grad_fn=<NegBackward>) critic loss  tensor(508.4304, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "Episode # 5\n",
      "min reward episode:  -1.1918594883543339\n",
      "max reward episode:  0.41788420575717206\n",
      "total loss  tensor(4204109.5497, dtype=torch.float64, grad_fn=<AddBackward0>) actor loss  tensor(4203612.2214, dtype=torch.float64, grad_fn=<NegBackward>) critic loss  tensor(497.3283, dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "Episode # 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3a95f2e6ecf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximumNrOfCPUs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimumNrOfCPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrewards_episodes_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_episodes_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_episodes_max\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8170add2feb4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, learning_rate, state_size, action_size, hidden_size, max_episodes, gamma)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master_thesis/Codebase/5growth-scaling/ActorCriticModel.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_size = 1 #Dimensions of amount of cars incoming, so 1?\n",
    "action_size = train_env.maximumNrOfCPUs - train_env.minimumNrOfCPUs\n",
    "rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max = \\\n",
    "        train(train_env, learning_rate, state_size, action_size, hidden_size, max_episodes, gamma)\n",
    "\n",
    "\n",
    "visualize(rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = TorinoEnvironment()\n",
    "# env.plotWork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorCriticModel.Actor(state_size, action_size, hidden_size).double()\n",
    "actor.load_state_dict(torch.load('actor_state_dict'))\n",
    "actor.eval()\n",
    "\n",
    "critic = ActorCriticModel.Critic(state_size, action_size, hidden_size).double()\n",
    "critic.load_state_dict(torch.load('critic_state_dict'))\n",
    "critic.eval()\n",
    "\n",
    "test_env.resetState()\n",
    "traceNrOfCPUs = np.zeros((test_env.duration,))\n",
    "traceMaxCPUload = np.zeros((test_env.duration,))\n",
    "\n",
    "#nrOfCPUs = 21 # best setup according to TorinoScalingConstant.ipynb\n",
    "\n",
    "state = np.array([test_env.minimumNrOfCPUs])\n",
    "rewards = []\n",
    "delays = []\n",
    "\n",
    "time = 0\n",
    "while not test_env.stop:\n",
    "    state = torch.from_numpy(state).double()\n",
    "    action_dist = actor.forward(state)\n",
    "    value = critic.forward(state)\n",
    "    action = np.random.choice(action_size, p=np.squeeze(action_dist.detach().numpy()))\n",
    "    state = np.array([test_env.evolveState(nrOfCPUs=action)])\n",
    "    \n",
    "    print(action)\n",
    "    print(action_dist)\n",
    "    \n",
    "    reward = test_env.getReward()\n",
    "    rewards.append(reward)    \n",
    "    traceNrOfCPUs[time] = state\n",
    "    traceMaxCPUload[time] = np.max(test_env.monitorState('CPUload'))\n",
    "    \n",
    "    delays.append(test_env.delay)\n",
    "    \n",
    "    time += 1\n",
    "\n",
    "rewards_benchmark = []\n",
    "test_env.resetState()\n",
    "nr_of_CPUs = 21\n",
    "while not test_env.stop:\n",
    "    state = nr_of_CPUs\n",
    "    test_env.evolveState(nrOfCPUs=action)\n",
    "    rewards_benchmark.append(test_env.getReward())\n",
    "\n",
    "# plot figures\n",
    "smoothed_traceNrOfCPUs = pd.Series.rolling(pd.Series(traceNrOfCPUs), 200).mean()\n",
    "smoothed_traceNrOfCPUs = [elem for elem in smoothed_traceNrOfCPUs]\n",
    "\n",
    "plt.figure(figsize = (16,4))\n",
    "plt.xlabel('time [5m interval]')\n",
    "plt.ylabel('nr of CPUs')\n",
    "plt.plot(traceNrOfCPUs)\n",
    "print(f'average number of CPUs {np.mean(traceNrOfCPUs):.1f}')\n",
    "\n",
    "smoothed_traceMaxCPUload = pd.Series.rolling(pd.Series(traceMaxCPUload), 200).mean()\n",
    "smoothed_traceMaxCPUload = [elem for elem in smoothed_traceMaxCPUload]\n",
    "\n",
    "plt.figure(figsize = (16,4))\n",
    "plt.xlabel('time [5m interval]')\n",
    "plt.ylabel('maximum CPU load')\n",
    "plt.plot(traceMaxCPUload)\n",
    "\n",
    "smoothed_delays = pd.Series.rolling(pd.Series(delays), 200).mean()\n",
    "smoothed_delays = [elem for elem in smoothed_delays]\n",
    "\n",
    "plt.figure(figsize= (16,4))\n",
    "plt.xlabel('time [5m interval]')\n",
    "plt.ylabel('Delays')\n",
    "plt.plot(smoothed_delays)\n",
    "\n",
    "smoothed_rewards = pd.Series.rolling(pd.Series(rewards), 200).mean()\n",
    "smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "\n",
    "plt.figure(figsize= (16,4))\n",
    "plt.xlabel('time [5m interval]')\n",
    "plt.ylabel('Rewards')\n",
    "plt.plot(rewards)\n",
    "\n",
    "smoothed_rewards_benchmark = pd.Series.rolling(pd.Series(rewards_benchmark), 200).mean()\n",
    "smoothed_rewards_benchmark = [elem for elem in smoothed_rewards_benchmark]\n",
    "\n",
    "plt.figure(figsize= (16,4))\n",
    "plt.xlabel('time [5m interval]')\n",
    "plt.ylabel('Rewards benchmark')\n",
    "plt.plot(rewards_benchmark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
