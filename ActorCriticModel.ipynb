{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acute-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size, temperature=1):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU()\n",
    "            )\n",
    "\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "            )\n",
    "\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            )\n",
    "\n",
    "        # maybe dim=1 if batchsize is larger than 1\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        action_dist = self.actor_head(x)\n",
    "        action_dist = self.softmax(action_dist/self.temperature)\n",
    "        value = self.critic_head(x)\n",
    "        return action_dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size , hidden_size, temperature=1):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, action_size)\n",
    "        self.relu = nn.ELU()\n",
    "        # maybe dim=1 if batchsize is larger than 1\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_dist = self.relu(self.linear1(state))\n",
    "        action_dist = self.relu(self.linear2(action_dist))\n",
    "        action_dist = self.relu(self.linear3(action_dist))\n",
    "        action_dist = self.softmax(self.linear4(action_dist)/self.temperature)        \n",
    "        return action_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "younger-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ELU()\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.relu(self.linear1(state))\n",
    "        value = self.relu(self.linear2(value))\n",
    "        value = self.relu(self.linear3(value))\n",
    "        value = self.linear4(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regulation-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(action_probs, values, rewards, gamma):\n",
    "    discounted_sum = 0.0\n",
    "    duration = len(rewards)\n",
    "    returns = torch.zeros((duration))\n",
    "    rewards = torch.flip(rewards, [0])\n",
    "\n",
    "    for i in range(duration):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        returns[i] = discounted_sum\n",
    "\n",
    "    returns = torch.flip(returns, [0])\n",
    "\n",
    "    advantage = returns - values.detach()\n",
    "    \n",
    "    log_action_probs = torch.log(action_probs)\n",
    "    loss_actor = torch.mean(-log_action_probs * advantage)\n",
    "    \n",
    "    loss_critic = nn.functional.mse_loss(values, returns)\n",
    "\n",
    "    return loss_actor, loss_critic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
