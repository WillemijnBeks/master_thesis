{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dependent-adjustment",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FiftyStations_OpenAIGym_wrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-92b653faee9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mFiftyStations_OpenAIGym_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorino_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FiftyStations_OpenAIGym_wrapper'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import import_ipynb\n",
    "import ActorCriticModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from FiftyStations_OpenAIGym_wrapper import Torino_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "learning_rate_actor = 1e-3\n",
    "learning_rate_critic = 1e-3\n",
    "max_episodes = 200\n",
    "gamma = 0.9\n",
    "\n",
    "action_size = 3\n",
    "actions = [-1, 0, 1]\n",
    "state_size = 2\n",
    "\n",
    "train_env, test_env = TorinoEnvironment(), TorinoEnvironment()\n",
    "\n",
    "train_env.head(.8)\n",
    "test_env.tail(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_work = TorinoEnvironment().getWork()\n",
    "# largest_diff = 0\n",
    "# index = 0\n",
    "# for i in range(len(env_work) - 1):\n",
    "#     if abs(env_work[i] - env_work[i+1]) > largest_diff:\n",
    "#         largest_diff = abs(env_work[i] - env_work[i+1])\n",
    "#         index = i\n",
    "# print(\"Largest difference in work at index\", i, \"with a difference of \", largest_diff[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    logx = torch.log(x)\n",
    "    Hx = torch.sum(x*logx)\n",
    "    return Hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, learning_rate_actor, learning_rate_critic, state_size, action_size, hidden_size, max_episodes, gamma, load_models=False):\n",
    "    actor = ActorCriticModel.Actor(state_size, action_size, hidden_size, temperature=0.9)\n",
    "    critic = ActorCriticModel.Critic(state_size, action_size, hidden_size)\n",
    "    print(actor)\n",
    "    print(critic)\n",
    "    \n",
    "    if load_models:\n",
    "        actor.load_state_dict(torch.load('actor_state_dict_EUCNC_compare_results'))\n",
    "        critic.load_state_dict(torch.load('critic_state_dict_EUCNC_compare_results'))\n",
    "\n",
    "    optimizer_actor = torch.optim.Adam(actor.parameters(), lr=learning_rate_actor) # Consider if Adam is best optimizer here\n",
    "    optimizer_critic = torch.optim.Adam(critic.parameters(), lr=learning_rate_critic)\n",
    "    \n",
    "    rewards_episodes_avg = []\n",
    "    rewards_episodes_min = []\n",
    "    rewards_episodes_max = []\n",
    "\n",
    "    losses_actor = []\n",
    "    losses_critic = []\n",
    "\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        print(\"#################\")\n",
    "        print(\"## Episode %03d ##\" % episode)\n",
    "        print(\"#################\")\n",
    "        print(\"[TRAINING]\")\n",
    "        env.resetState()\n",
    "\n",
    "        W = int(env.monitorState('instant_work'))\n",
    "        N = W + 1\n",
    "        state  = np.array((W, N))\n",
    "\n",
    "        rewards = []\n",
    "        values = []\n",
    "        action_probs = []\n",
    "        entropies = []\n",
    "        k = 1\n",
    "\n",
    "        #for step in tqdm(range(env.duration)):\n",
    "        for step in tqdm(range(500)):\n",
    "            state = torch.from_numpy(state).float()\n",
    "            action_dist = actor.forward(state)\n",
    "\n",
    "            if action_dist.isnan().any():\n",
    "                break\n",
    "    \n",
    "            value = critic.forward(state)\n",
    "            values.append(value)\n",
    "            \n",
    "            action = np.random.choice(actions, p=np.squeeze(action_dist.detach().cpu().numpy()))\n",
    "            action_probs.append(action_dist[actions.index(action)])\n",
    "\n",
    "            entropies.append(entropy(action_dist))\n",
    "\n",
    "            env.evolveState(nrOfCPUs=int(state.cpu().numpy()[1]) + action)\n",
    "            \n",
    "            reward = env.getReward()\n",
    "            rewards.append(reward)\n",
    "\n",
    "            W = int(env.monitorState('instant_work'))\n",
    "            N = len(env.monitorState('CPUload'))\n",
    "            state = np.array((W, N))\n",
    "\n",
    "        print(\"min reward episode: \", np.amin(rewards))\n",
    "        print(\"max reward episode: \", np.amax(rewards))\n",
    "        print(\"avg reward episode: \", np.mean(rewards))\n",
    "\n",
    "        rewards_episodes_avg.append(np.mean(rewards)) \n",
    "        rewards_episodes_min.append(np.amin(rewards))\n",
    "        rewards_episodes_max.append(np.amax(rewards))\n",
    "\n",
    "        \n",
    "        optimizer_actor.zero_grad()\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss_actor, loss_critic = ActorCriticModel.calc_loss(torch.stack(action_probs), \n",
    "                                          torch.cat(values), \n",
    "                                          torch.tensor(rewards).float(),\n",
    "                                          gamma)\n",
    "        \n",
    "        loss_entropy = torch.mean(torch.stack(entropies))\n",
    "        loss = loss_actor + loss_critic + loss_entropy*k\n",
    "        print(\"total_loss \", loss.item(), \"| actor_loss \", loss_actor.item(), \"| critic_loss \", loss_critic.item())\n",
    "        losses_actor.append(loss_actor.item())\n",
    "        losses_critic.append(loss_critic.item())\n",
    "        loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        torch.save(actor.state_dict(), 'actor_state_dict_EUCNC_compare_results')\n",
    "        torch.save(critic.state_dict(), 'critic_state_dict_EUCNC_compare_results')\n",
    "\n",
    "        if episode%50 == 0:\n",
    "            val(test_env)\n",
    "    \n",
    "    plt.plot(losses_actor, label=\"loss actor\")\n",
    "    plt.legend()\n",
    "#    plt.savefig(\"loss_actor_EUCNC_comparison\")\n",
    "#    plt.show()\n",
    "    plt.plot(losses_critic, label=\"loss critic\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_critic_EUCNC_comparison\")\n",
    "    plt.show()\n",
    "\n",
    "    return rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(env):\n",
    "\n",
    "    print(\"[VALIDATION]\")\n",
    "    # Load models\n",
    "    actor = ActorCriticModel.Actor(state_size, action_size, hidden_size)\n",
    "    actor.load_state_dict(torch.load('actor_state_dict_EUCNC_compare_results'))\n",
    "    actor.eval()\n",
    "    \n",
    "    critic = ActorCriticModel.Critic(state_size, action_size, hidden_size)\n",
    "    critic.load_state_dict(torch.load('critic_state_dict_EUCNC_compare_results'))\n",
    "    critic.eval()\n",
    "    \n",
    "    rewards_episodes_avg = []\n",
    "    rewards_episodes_min = []\n",
    "    rewards_episodes_max = []\n",
    "\n",
    "    losses_actor = []\n",
    "    losses_critic = []\n",
    "    \n",
    "    env.resetState()\n",
    "\n",
    "    W = int(env.monitorState('instant_work'))\n",
    "    N = W + 1\n",
    "    state  = np.array((W, N))\n",
    "\n",
    "    rewards = []\n",
    "    values = []\n",
    "    action_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in tqdm(range(env.duration)):\n",
    "            state = torch.from_numpy(state).float()\n",
    "            action_dist = actor.forward(state)\n",
    "    \n",
    "            if action_dist.isnan().any():\n",
    "                break\n",
    "    \n",
    "            value = critic.forward(state)\n",
    "            values.append(value)\n",
    "            \n",
    "            #action = np.random.choice(actions, p=np.squeeze(action_dist.detach().cpu().numpy()))\n",
    "            action = actions[np.argmax(action_dist.detach().cpu().numpy())]\n",
    "            action_probs.append(action_dist[actions.index(action)])\n",
    "    \n",
    "            env.evolveState(nrOfCPUs=int(state.cpu().numpy()[1]) + action)\n",
    "            \n",
    "            reward = env.getReward()\n",
    "            rewards.append(reward)\n",
    "    \n",
    "            W = int(env.monitorState('instant_work'))\n",
    "            N = len(env.monitorState('CPUload'))\n",
    "            state = np.array((W, N))\n",
    "    \n",
    "        print(\"min reward episode: \", np.amin(rewards))\n",
    "        print(\"max reward episode: \", np.amax(rewards))\n",
    "        print(\"avg reward episode: \", np.mean(rewards))\n",
    "\n",
    "        rewards_episodes_avg.append(np.mean(rewards)) \n",
    "        rewards_episodes_min.append(np.amin(rewards))\n",
    "        rewards_episodes_max.append(np.amax(rewards))\n",
    "\n",
    "        loss_actor, loss_critic = ActorCriticModel.calc_loss(torch.stack(action_probs), \n",
    "                                          torch.cat(values), \n",
    "                                          torch.tensor(rewards).float(),\n",
    "                                          gamma)\n",
    "        \n",
    "        loss = loss_actor + loss_critic\n",
    "        print(\"total_loss \", loss.item(), \"| actor_loss \", loss_actor.item(), \"| critic_loss \", loss_critic.item())\n",
    "        losses_actor.append(loss_actor.item())\n",
    "        losses_critic.append(loss_critic.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, save=True):\n",
    "\n",
    "    # Load models\n",
    "    actor = ActorCriticModel.Actor(state_size, action_size, hidden_size)\n",
    "    actor.load_state_dict(torch.load('actor_state_dict_EUCNC_compare_results'))\n",
    "    actor.eval()\n",
    "    \n",
    "    critic = ActorCriticModel.Critic(state_size, action_size, hidden_size)\n",
    "    critic.load_state_dict(torch.load('critic_state_dict_EUCNC_compare_results'))\n",
    "    critic.eval()\n",
    "    \n",
    "    # Run test environment\n",
    "    env.resetState()\n",
    "    traceNrOfCPUs = np.zeros((env.duration,))\n",
    "    traceMaxCPUload = np.zeros((env.duration,))\n",
    "    traceSumBacklog = np.zeros((env.duration,))\n",
    "    traceReward = np.zeros((env.duration,))\n",
    "    \n",
    "    W = int(env.monitorState('instant_work'))\n",
    "    N = W + 1\n",
    "    state  = np.array((W, N))\n",
    "    rewards = []\n",
    "    delays = []\n",
    "    time = 0\n",
    "\n",
    "    #while not env.stop:\n",
    "    for step in tqdm(range(env.duration)):\n",
    "\n",
    "        state = torch.from_numpy(state).float()\n",
    "        action_dist = actor.forward(state)\n",
    "        value = critic.forward(state)\n",
    "        #action = np.random.choice(actions, p=np.squeeze(action_dist.detach().numpy()))\n",
    "        action = actions[np.argmax(action_dist.detach().cpu().numpy())]\n",
    "        \n",
    "        env.evolveState(nrOfCPUs=int(state.numpy()[1]) + action)\n",
    "          \n",
    "        reward = env.getReward()\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        W = int(env.monitorState('instant_work'))\n",
    "        N = len(env.monitorState('CPUload'))\n",
    "        state = np.array((W, N))    \n",
    "     \n",
    "        traceNrOfCPUs[time] = state[1]\n",
    "        traceMaxCPUload[time] = np.max(env.monitorState('CPUload'))\n",
    "        traceSumBacklog[time] = np.sum(env.monitorState('backlog'))\n",
    "        traceReward[time] = env.getReward()\n",
    "        \n",
    "        delays.append(env.delay)\n",
    "        \n",
    "        time += 1\n",
    "\n",
    "    if save:\n",
    "        #add in all other plots to compare\n",
    "        pd.DataFrame({\n",
    "            'nrOfCPUs': traceNrOfCPUs,\n",
    "            'maxCPUload': traceMaxCPUload,\n",
    "            'sumBacklog': traceSumBacklog,\n",
    "            'reward': traceReward,\n",
    "        }).to_csv('A2C.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max):    \n",
    "    #smoothed_rewards = pd.Series.rolling(pd.Series(rewards_episodes), 10).mean() # hier zou alle rewards weer meegenomen moeten worden\n",
    "    #smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "    \n",
    "    plt.plot(rewards_episodes_avg, label=\"Average reward per episode\")\n",
    "    plt.plot(rewards_episodes_min, label=\"Minimum reward per episode\")\n",
    "    plt.plot(rewards_episodes_max, label=\"Maximum reward per episode\")\n",
    "    #plt.plot(smoothed_rewards)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.savefig(\"rewards_episodes_EUCNCcomparison\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max = train(train_env, learning_rate_actor, learning_rate_critic, state_size, action_size, hidden_size, max_episodes, gamma, load_models=False)\n",
    "\n",
    "# test\n",
    "val(test_env)\n",
    "test(test_env)\n",
    "\n",
    "visualize(rewards_episodes_avg, rewards_episodes_min, rewards_episodes_max)\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
